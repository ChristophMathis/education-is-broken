# Methodological Note

This book draws on evidence of widely varying quality. Because education reform attracts strong claims from parties with strong incentives -- universities defending their models, vendors selling platforms, governments justifying policies -- readers deserve transparency about what kind of evidence underpins any given claim.

Throughout the manuscript, we have tried to signal evidence quality inline, using parenthetical caveats where the source or methodology matters. This note explains the hierarchy we applied.

## Evidence Hierarchy

### Tier 1: Meta-analyses and large-scale randomized controlled trials (RCTs)

These represent the strongest form of evidence. They include systematic reviews that aggregate findings across multiple studies and large-scale experiments with random assignment, control groups, and independent evaluation. Examples in this book include the meta-analytic evidence on active learning versus lectures and the 2015 meta-review of competency-based education. When Tier 1 evidence exists, we treat its conclusions as the most reliable available -- though even meta-analyses have limitations (publication bias, heterogeneity across studies, and dependence on the quality of included research).

### Tier 2: Quasi-experimental studies and multi-site evaluations

These studies compare outcomes across groups without full randomization, often using statistical controls, matched samples, or natural experiments. Multi-site evaluations that track outcomes across several institutions or countries also fall here. Examples include cross-national employer surveys with large samples and OECD comparative analyses. Tier 2 evidence is informative but requires caution about confounding variables and selection effects.

### Tier 3: Single-institution pilots, case studies, and observational data

Much of the evidence on innovative education models falls in this tier. A single university reporting outcomes from a new program, a pilot study at one institution, or observational data from a specific implementation are valuable for generating hypotheses and illustrating possibilities, but they cannot establish generalizable causal claims. Selection effects are a persistent concern: institutions that pilot new models often serve different populations than those that do not, making direct comparisons unreliable. Examples in this book include Minerva's internal learning-gains studies, ECIU's micro-credential pilot, and Utrecht University's challenge-based learning observations.

### Tier 4: Vendor-reported data, testimonials, and marketing claims

This is the most common -- and least reliable -- source of evidence in the education technology and corporate learning space. Vendor-reported data includes figures published by companies about their own products (e.g., platform adoption rates, skill acquisition speed, retention improvements) and institutional self-reports designed for marketing or fundraising. These figures are produced by parties with direct financial or reputational incentives to present positive results. They typically lack independent verification, transparent methodology, or control groups. Examples in this book include Guild Education's mobility and retention figures, AI learning platform speed-of-acquisition claims, and corporate case studies on predictive analytics.

We do not dismiss Tier 4 evidence entirely -- in many areas of educational innovation, it is the only data available, and ignoring it would leave the picture incomplete. But we flag it explicitly so that readers can calibrate their confidence accordingly.

## How to Read the Evidence in This Book

Where a claim is supported by Tier 1 or Tier 2 evidence, we generally present it without extended qualification. Where a claim rests primarily on Tier 3 or Tier 4 evidence, we have added inline caveats indicating the source type -- for example, "(vendor-reported)," "(single-institution pilot)," "(institutional self-report)," or "(industry survey; definitions vary)."

These caveats are not meant to discredit the claims but to help readers distinguish between what is well-established and what is promising but unverified. In a field where bold claims travel faster than rigorous evidence, this distinction matters.

The evidence base for educational innovation is improving, but it remains thinner than the confidence of the claims made about it. We have tried to be honest about that throughout.

## The Diagnosis–Prescription Evidence Gap

Transparency about evidence quality reveals a structural asymmetry in this book that readers should weigh carefully. The diagnostic claims -- that higher education costs are rising unsustainably, that credential inflation erodes signal quality, that employer demand is shifting toward demonstrated capability, that equity gaps are widening -- draw primarily on Tier 1–2 evidence: cross-national labor force data, OECD comparative statistics, large-scale employer surveys with independent replication, and econometric analyses of returns to education. These claims rest on solid empirical ground.

The prescriptive claims -- that competency-based education works at scale, that employer-sponsored learning expands opportunity, that peer-driven models like 42 School produce superior outcomes, that AI tutoring accelerates learning by 30–60%, that European university alliances represent a viable alternative architecture -- draw predominantly on Tier 3–4 evidence: single-institution pilots, vendor-reported data, institutional self-reports, and infrastructure demonstrations without outcome evaluation. This is not because the authors chose weak evidence. It is because stronger evidence does not yet exist for most of these innovations. The models are too new, too few, or too heterogeneous for the kind of multi-site, controlled, longitudinal studies that would constitute Tier 1–2 evidence.

The implication is that this book's diagnosis is substantially stronger than its reform agenda. We are confident that the current system is failing in the ways described. We are less confident that the specific models and mechanisms we discuss will deliver the outcomes their proponents claim. The honest framing is: these are plausible, promising, and in some cases partially validated directions -- not proven solutions. Where we recommend investment or institutional redesign, we do so on the basis of directional evidence and design logic, not established efficacy.

This gap is not unusual in a field undergoing rapid structural change -- the evidence inevitably lags the innovation. But it means that readers, policymakers, and institutional leaders who act on the prescriptive chapters should do so with appropriate caution: invest in pilot evaluation, demand independent outcome data, and resist the temptation to scale models before they have been tested beyond their founding context.

## Geographic Scope and Its Limits

The evidence and examples in this book draw predominantly from OECD countries, with a center of gravity in Europe and North America. This is partly a function of data availability -- large-scale labor market studies, employer surveys, and institutional outcome data are most systematically collected in these regions -- and partly a function of the authors' own professional networks and expertise.

The imbalance is uneven across chapters. The analysis of funding models, regulatory frameworks, and public governance (Chapters 9–11) leans heavily on European systems -- Nordic, German, Dutch, Austrian -- where the relationship between state funding and institutional autonomy is most explicitly codified. The discussion of alternative credentialing and employer-led learning (Chapters 6–7, Chapter 9) draws more heavily on US and Anglophone examples, where market-driven experimentation has gone furthest. The Global South appears primarily through targeted examples -- Rwanda's coding academies, India's skill certification schemes, Singapore's SkillsFuture -- rather than through sustained structural analysis.

Readers should be aware of this geographic center of gravity. The diagnostic claims about credential inflation, cost escalation, and employer demand shifts are supported by data from multiple regions and are likely to hold broadly. The prescriptive proposals -- particularly around governance reform, funding architecture, and quality assurance -- may transfer less cleanly to systems with fundamentally different institutional histories, labor market structures, or state capacities. We have tried to flag where a recommendation is context-dependent, but the default frame remains OECD-centric, and readers working in or on other systems should calibrate accordingly.
