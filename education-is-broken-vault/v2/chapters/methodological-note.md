# Methodological Note

This book draws on evidence of widely varying quality. Because education reform attracts strong claims from parties with strong incentives -- universities defending their models, vendors selling platforms, governments justifying policies -- readers deserve transparency about what kind of evidence underpins any given claim.

Throughout the manuscript, we have tried to signal evidence quality inline, using parenthetical caveats where the source or methodology matters. This note explains the hierarchy we applied.

---

## Evidence Hierarchy

### Tier 1: Meta-analyses and large-scale randomised controlled trials (RCTs)

These represent the strongest form of evidence. They include systematic reviews that aggregate findings across multiple studies and large-scale experiments with random assignment, control groups, and independent evaluation. Examples in this book include the meta-analytic evidence on active learning versus lectures and the 2015 meta-review of competency-based education. When Tier 1 evidence exists, we treat its conclusions as the most reliable available -- though even meta-analyses have limitations (publication bias, heterogeneity across studies, and dependence on the quality of included research).

### Tier 2: Quasi-experimental studies and multi-site evaluations

These studies compare outcomes across groups without full randomisation, often using statistical controls, matched samples, or natural experiments. Multi-site evaluations that track outcomes across several institutions or countries also fall here. Examples include cross-national employer surveys with large samples and OECD comparative analyses. Tier 2 evidence is informative but requires caution about confounding variables and selection effects.

### Tier 3: Single-institution pilots, case studies, and observational data

Much of the evidence on innovative education models falls in this tier. A single university reporting outcomes from a new program, a pilot study at one institution, or observational data from a specific implementation are valuable for generating hypotheses and illustrating possibilities, but they cannot establish generalisable causal claims. Selection effects are a persistent concern: institutions that pilot new models often serve different populations than those that do not, making direct comparisons unreliable. Examples in this book include Minerva's internal learning-gains studies, ECIU's micro-credential pilot, and Utrecht University's challenge-based learning observations.

### Tier 4: Vendor-reported data, testimonials, and marketing claims

This is the most common -- and least reliable -- source of evidence in the education technology and corporate learning space. Vendor-reported data includes figures published by companies about their own products (e.g., platform adoption rates, skill acquisition speed, retention improvements) and institutional self-reports designed for marketing or fundraising. These figures are produced by parties with direct financial or reputational incentives to present positive results. They typically lack independent verification, transparent methodology, or control groups. Examples in this book include Guild Education's mobility and retention figures, AI learning platform speed-of-acquisition claims, and corporate case studies on predictive analytics.

We do not dismiss Tier 4 evidence entirely -- in many areas of educational innovation, it is the only data available, and ignoring it would leave the picture incomplete. But we flag it explicitly so that readers can calibrate their confidence accordingly.

---

## How to Read the Evidence in This Book

Where a claim is supported by Tier 1 or Tier 2 evidence, we generally present it without extended qualification. Where a claim rests primarily on Tier 3 or Tier 4 evidence, we have added inline caveats indicating the source type -- for example, "(vendor-reported)," "(single-institution pilot)," "(institutional self-report)," or "(industry survey; definitions vary)."

These caveats are not meant to discredit the claims but to help readers distinguish between what is well-established and what is promising but unverified. In a field where bold claims travel faster than rigorous evidence, this distinction matters.

The evidence base for educational innovation is improving, but it remains thinner than the confidence of the claims made about it. We have tried to be honest about that throughout.

## Annotated Source List

This note explains the evidence hierarchy applied throughout the book. It references the same sources catalogued in the chapter-level reference lists and in references.bib.

## References

No new references are introduced in this note. All cited sources appear in the chapter-level reference lists and in references.bib.
