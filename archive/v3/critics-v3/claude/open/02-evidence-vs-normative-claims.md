# Proposal 2: Evidence Use vs Normative Claims

## Evaluation

The critique is **valid and worth taking seriously**, though the severity depends on how systematically the tension appears across chapters.

### What the critic sees correctly:

1. **Real tension exists.** The methodological note explicitly signals Tier 3 and Tier 4 sources (single-institution pilots, vendor data), but several chapters make broad predictive claims built on exactly these sources. For example:
   - Chapter 05 projects the demotion of degrees and the emergence of a "signalling stack" as established frameworks, but the evidence supporting this is mostly:
     - 2024-2025 employer surveys (Tier 2–3, useful but not definitive)
     - ECIU and other single-institution pilot implementations (Tier 3)
     - Vendor-reported data on AI assessment adoption (Tier 4)
   - Chapter 01 describes "extraordinary new possibilities" (AI tutoring, micro-courses, employer-funded learning) without clearly distinguishing between what is actually happening, what is being piloted, and what is industry projection.

2. **The evidence hierarchy creates legitimate expectations.** By documenting a rigorous four-tier system, you invite readers to calibrate their confidence. When a chapter later says "the degree is being replaced" (a normative claim about what is happening) based on Tier 3–4 evidence, the implicit contract is broken. The caveats are there, but they are scattered in parentheses rather than structural.

3. **Extrapolation happens.** Chapter 05 explicitly moves from "81% of employers report using skills-based approaches" (observable practice) to "the degree's monopoly is ending" (normative claim) to multi-decade scenarios ("over the next 20 years, the degree would substantially weaken outside regulated professions"). Each step is reasonable, but the confidence level inflates as you move away from the data.

### Why this matters:

The book's credibility rests on the contrast: it provides evidence-based argument in a field full of hype. If readers sense you are making the same moves as consultancy reports -- citing data but extrapolating beyond it, using careful methodological notes to signal rigour while still engaging in speculation -- they will discount everything, including the genuinely solid claims (like the skill gaps, the cost crisis, and the equity failures).

### Severity assessment:

**Moderate to high.** Not all chapters have this problem equally. Chapter 01 is mostly phenomenological (describing what people experience and what the system looks like now), so the tension is less acute. Chapter 05 is the most exposed: it moves from survey data to historical narrative to speculative frameworks, and the reader is left unsure which is grounded and which is projection.

This is not a reason to cut the speculative content. The book needs to think forward. But it does need to make those moves visible.

---

## Proposed Solution

### Template: "Evidence vs Conjecture" Box

Include a standing box (sidebar) at the end of each chapter (or strategically within longer chapters) that:

1. **Names the claim:** Restates the chapter's main forward-looking argument in plain language.
2. **Names the evidence:** States what tier of evidence supports each part.
3. **Names the gap:** Acknowledges explicitly what evidence would be needed to confirm the claim, and what would change if that evidence contradicted it.
4. **Stakes the confidence level:** Uses plain language ("well-grounded," "plausible given current trends," "speculative extrapolation") rather than probabilistic language that feels false.

### Template Format:

```
---
### Evidence vs Conjecture: [Claim Title]

**The claim:**
[One-sentence restatement of the chapter's main forward-looking argument or prediction]

**What supports it:**
- [First part of claim] is based on [Tier X: type of evidence, e.g., "Tier 2 cross-national employer survey data from TestGorilla and LinkedIn (2025)"].
- [Second part of claim] is based on [Tier X: type of evidence].
- [Third part of claim] is a [synthesis / inference / scenario] not drawn directly from published research.

**What would change it:**
- If [evidence type] showed that [key assumption] is wrong, this claim would need substantial revision.
- Specifically, we assume [underlying premise]. Large-scale longitudinal data tracking hiring outcomes over 5-10 years could test this.
- The scenario for 20-year outcomes assumes [key condition]. Geopolitical or regulatory shifts could alter this trajectory significantly.

**Confidence level:**
[Well-grounded in current data / Plausible extrapolation from observed trends / Speculative but internally consistent]

---
```

### Example Application: Chapter 05 (Degrees, Skills, and the Signalling Stack)

Here is how this would look applied to Chapter 05:

```
---
### Evidence vs Conjecture: The Signalling Stack and the Demotion of Degrees

**The claim:**
The degree is shifting from a gate to a preference, becoming one layer in a multilevel "signalling stack" where employers increasingly weight skills, portfolios, and AI-derived behavioural signals alongside or instead of traditional credentials.

**What supports it:**

**Layer 1 — Observed employer behaviour (Tier 2–3):**
- 81% of employers globally report using skills-based hiring approaches (up from 73% in 2023), per TestGorilla and LinkedIn surveys. This is observational data from large samples, but the survey definitions and validity vary.
- 77% of European companies plan to prioritise competency-based recruiting in 2026, per recruiters' stated intentions. This is forward-looking assertion by employers, not yet observed practice.
- Skills-based hiring expands candidate pools by 10–17x (UK, US, Gen Z), per TestGorilla. This assumes that candidates without traditional degrees are actually being hired; follow-on hiring data is limited.
- 41% of talent acquisition teams use AI-powered skills assessments; 83% use gamified assessments. These are adoption metrics (Tier 3–4), not proof that these tools outperform traditional hiring or produce better outcomes.

**Layer 2 — Existing portfolio-driven hiring (Tier 3):**
- Software development, design, and data science already show portfolio-first hiring practices (single-firm case studies and industry observation, Tier 3).
- Tech giants (Google, Apple, IBM) have publicly dropped degree requirements. These are institutional decisions by specific companies; we lack systematic data on whether this is reversing, deepening, or stalling.
- The 42 School model spans 54 campuses; employers report graduates excel at independent problem-solving (Tier 3, single-provider data).

**Layer 3 — Emerging infrastructure (Tier 3):**
- European blockchain credential pilots (ECIU, Blockcerts, Europass) demonstrate technical feasibility (Tier 3, pilot implementation).
- European Commission joint degree label rolling out mid-2026 is announced policy, not yet evaluated (policy intent, pre-implementation).

**Layer 4 — The speculative layer:**
- The four-layer signalling stack framework itself is the authors' synthesis, not drawn from published empirical research.
- The 5-, 10-, and 20-year scenarios are plausible extrapolations of current trends, not predictions based on forecasting models or historical precedent.
- The claim that behavioural signals could become a primary hiring signal is hypothetical; no data shows this is currently happening at scale.

**What would change it:**

- If longitudinal employer hiring data (5–10 years) showed that skills-based hiring has plateaued or reversed, or that degree requirements have rebounded, the trend narrative would need rethinking.
- If the credential inflation problem (1 million+ credentials in the U.S., 46% of employers unable to assess them) worsens rather than resolves, the signalling stack risks fragmenting instead of maturing.
- If regulated professions (law, medicine, nursing, engineering) do not introduce competency-based pathways within 15 years, the demotion of degrees remains confined to already-flexible sectors.
- If AI assessment tools do not outperform traditional hiring on standard labour-market outcomes (retention, performance ratings, diversity), the adoption of these tools may be vendor-driven rather than outcome-driven.
- If privacy regulation or bias scandals slow the deployment of behavioural analytics in hiring, the Layer 4 scenario becomes less plausible.

**Confidence level:**
- The 5-year scenario (hybrid world with skills-based supplements to degree requirements) is well-grounded in current observable trends.
- The 10-year scenario (maturation of quality assurance frameworks, blockchain wallets, possible regulatory reform) is a plausible extrapolation, contingent on sustained investment and policy alignment.
- The 20-year scenario (degree's monopoly substantially weakened, behavioural signals as primary hiring signal) is speculative and contingent on major structural changes (regulatory reform, cultural shift, algorithmic trust) that are not guaranteed.

---
```

### How to apply across the book:

**Chapter 01 (What Exactly Is Broken?):**
This chapter is mostly phenomenological and evidence-driven. The box here is smaller -- it would focus on the one speculative claim: "As AI-powered learning tools become more available, a new kind of inequality may be emerging." Ground this in current disparity research (well-supported), then acknowledge that the "use as thought partner vs. passive entertainment" distinction is a hypothesis about future behaviour, not observed fact.

**Chapters on innovation models (competency-based, campus-free, coding schools, etc.):**
These chapters rest heavily on Tier 3 evidence (Minerva's internal studies, ECIU pilots, 42 School case studies). The box would say: "These cases demonstrate proof of concept; they do not yet establish what happens when these models scale beyond their founding contexts." Name what evidence would be needed: cross-institutional comparisons, labour-market outcome tracking, equity audits.

**Chapters on AI and hiring:**
These chapters use a mix of Tier 2 (adoption surveys) and Tier 4 (vendor claims about accuracy and bias mitigation). The box would separate: "We know this many companies are experimenting. We do not yet know whether these tools improve hiring outcomes or create new forms of discrimination."

**Chapters on employer learning systems:**
These rest on Tier 3–4 (corporate case studies, vendor reports on platform outcomes). The box would clearly distinguish between "what companies are doing" (observed) and "whether it produces better workers" (claimed, often vendor-reported).

---

## Guidance on Implementation

### Where to place the box:

**End of chapter is preferred** -- it signals that you have finished the main argument and are now stepping back to reflect on the evidence. This creates a rhythm: narrative and evidence, then meta-commentary on the same.

**Within the chapter** works for very long chapters (like Chapter 05), where you might include a box after the "Short Term (5 Years)" section to separate grounded from speculative, then another box at the end.

### Tone:

Use Krishan's voice: direct, honest, no hedging. Not "one might argue" or "it is possible that" -- that's what made the methodological note feel like cover rather than transparency.

Try:
- "We don't yet know whether [X]."
- "This is extrapolation, not data."
- "If [evidence] contradicts [assumption], this whole argument flips."
- "We are confident in [grounded part]; we are speculating in [forward part]."

Not:
- "Some scholars suggest..."
- "It could be argued..."
- "While rigorous evidence is limited..."

### Benefit to the book:

1. **Credibility:** Readers will trust the speculative parts more if you explicitly mark them as speculative.
2. **Sharper argument:** Forcing yourself to name what evidence would change your mind often tightens the logic.
3. **No softening of the book's voice:** These boxes do not need to be cautious or equivocal. They can be clear and sharp: "Here is what we know. Here is what we are guessing. Here is what would prove us wrong."
4. **Reader agency:** You are giving readers the tools to evaluate your argument, not hiding behind methodological notes they may skip.

---

## Next Steps

1. **Start with Chapter 05** as the test case. This chapter is the most exposed to the critique and benefits most from the box.
2. **Iterate the template** based on what you learn. The first box will feel awkward; by the third, you will find the rhythm.
3. **Apply selectively.** Not every chapter needs a box. Chapters that stay close to observed phenomena (equity gaps, cost crisis, learning outcomes) may not need one. Chapters that project forward (futures of regulation, AI adoption, institutional models) definitely do.
4. **Consider combining with the annotated source list.** The source list already breaks down evidence by tier. The conjecture box complements it by saying "here is where the extrapolation happens and what would change it."

---
